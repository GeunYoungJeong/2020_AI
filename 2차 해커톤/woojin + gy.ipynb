{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"woojin + gy.ipynb","provenance":[{"file_id":"1PbrCo9OPB8DnS2i2ZswMQs6MqhrL1WX0","timestamp":1606380581059}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"RbUtNDek4RVm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606968583773,"user_tz":-540,"elapsed":21884,"user":{"displayName":"박우진","photoUrl":"","userId":"05832430705160492475"}},"outputId":"8dae559e-7d9c-408a-8dde-2a54d8c08719"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lK7R3r404hWn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606968643311,"user_tz":-540,"elapsed":5100,"user":{"displayName":"박우진","photoUrl":"","userId":"05832430705160492475"}},"outputId":"03f38bd0-b633-4323-953a-91fb8cadad7d"},"source":["!pip install pytorch-crf\n","!pip install seqeval==1.0.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n","Requirement already satisfied: seqeval==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.6/dist-packages (from seqeval==1.0.0) (0.23.2)\n","Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.6/dist-packages (from seqeval==1.0.0) (1.19.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (2.1.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (0.17.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N2Zh7KtVg-G_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606968690132,"user_tz":-540,"elapsed":32288,"user":{"displayName":"박우진","photoUrl":"","userId":"05832430705160492475"}},"outputId":"0184a241-adb6-493a-c96e-73fe7b33c544"},"source":["import os\n","import numpy as np\n","import torch\n","\n","# root_dir = \"/gdrive/My Drive/AI_Konkuk/해커톤 baseline\"\n","root_dir = \"./gdrive/MyDrive/data/hackaton\"\n","file_list = os.listdir('%s/npydata/'%(root_dir))\n","feature_dict = dict()\n","print(file_list)\n","\n","for file in file_list:\n","    path = '%s/npydata/%s'%(root_dir, file)\n","    if not os.path.isfile(path):\n","        continue\n","    if \"200\" not in file:\n","        continue\n","    feature_dict[file] = torch.from_numpy(np.load(path, allow_pickle=True))\n","    print(file, feature_dict[file].shape)\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['bigram_train_50.npy', 'bigram_test_50.npy', '.ipynb_checkpoints', 'pumsa_onehot.npy', 'trigram_train_100.npy', 'trigram_test_100.npy', 'pumsa_onehot_test.npy', 'pumsa_onehot_200.npy', 'pumsa_onehot_200_test.npy', 'trigram_train_100_200.npy', 'bigram_train_50_200.npy', 'trigram_test_100_200.npy', 'bigram_test_50_200.npy', 'pumsa_onehot_350.npy', 'pumsa_onehot_350_test.npy', 'bigram_train_50_350.npy', 'bigram_test_50_350.npy', 'trigram_test_100_350.npy', 'trigram_train_100_350.npy']\n","pumsa_onehot_200.npy torch.Size([7319, 200, 46])\n","pumsa_onehot_200_test.npy torch.Size([995, 200, 46])\n","trigram_train_100_200.npy torch.Size([7319, 200, 100])\n","bigram_train_50_200.npy torch.Size([7319, 200, 50])\n","trigram_test_100_200.npy torch.Size([995, 200, 100])\n","bigram_test_50_200.npy torch.Size([995, 200, 50])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B6rb8BwhZk_S"},"source":["#모델 코드"]},{"cell_type":"code","metadata":{"id":"cdBFg0kb4Urd"},"source":["import torch\n","import torch.nn as nn\n","from torchcrf import CRF\n","\n","from seqeval.metrics import classification_report\n","\n","\n","class RNN_CRF(nn.Module):\n","    def __init__(self, config):\n","        super(RNN_CRF, self).__init__()\n","\n","        # 전체 음절 개수\n","        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n","\n","        # 음절 임베딩 사이즈\n","        self.embedding_size = config[\"embedding_size\"]\n","\n","        # GRU 히든 사이즈\n","        self.hidden_size = config[\"hidden_size\"]\n","\n","        # 분류할 태그의 개수\n","        self.number_of_tags = config[\"number_of_tags\"]\n","\n","        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n","        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n","                                      embedding_dim=self.embedding_size,\n","                                      padding_idx=0)\n","        self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","        # Bi-GRU layer\n","        self.bi_gru = nn.LSTM(input_size = self.embedding_size+46,\n","                             hidden_size= self.hidden_size,\n","                             num_layers=1,\n","                             batch_first=True,\n","                             bidirectional=True)\n","        \n","        self.sec_bi_gru = nn.LSTM(input_size = self.embedding_size*3+150+46,\n","                                 hidden_size = self.hidden_size,\n","                                 num_layers=1,\n","                                 batch_first=True,\n","                                 bidirectional=True)\n","        # CRF layer\n","        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n","\n","        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n","        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n","        self.hidden2num_tag = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_tags)\n","\n","    def forward(self, inputs, labels=None, pumsa=None, inputs_ngram = None):\n","        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n","        eumjeol_inputs = self.embedding(inputs)\n","        eumjeol_inputs = torch.cat([eumjeol_inputs, pumsa], dim=-1)\n","        encoder_outputs, hidden_states = self.bi_gru(eumjeol_inputs)\n","\n","        # print('encoder',encoder_outputs.shape)\n","        # print('hidden',hidden_states.shape)\n","        # (batch_size, curr_max_length, hidden_size*2)\n","        d_hidden_outputs = self.dropout(encoder_outputs)\n","        inputs_ngram = inputs_ngram.type(torch.FloatTensor).cuda()\n","\n","        eumjeol_inputs = torch.cat([eumjeol_inputs, d_hidden_outputs, inputs_ngram], dim=-1)\n","\n","        encoder_outputs, hidden_states = self.sec_bi_gru(eumjeol_inputs)\n","        # print('d_hidden', d_hidden_outputs.shape)\n","        d_hidden_outputs = self.dropout(encoder_outputs)\n","        # cat_feature = torch.cat([d_hidden_outputs, inputs_ngram], dim = -1)\n","        # cat_feature = cat_feature.type(torch.DoubleTensor).cuda()\n","        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n","        logits = self.hidden2num_tag(d_hidden_outputs)\n","        # print('logits', logits.shape)\n","        if(labels is not None):\n","            log_likelihood = self.crf(emissions=logits,\n","                                      tags=labels,\n","                                      reduction=\"mean\")\n","\n","            loss = log_likelihood * -1.0\n","\n","            return loss\n","        else:\n","            output = self.crf.decode(emissions=logits)\n","            return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKBiytZGYShY"},"source":["#개체명 사전으로 진행하기[병,근]"]},{"cell_type":"code","metadata":{"id":"NJsDegUGxZuQ","cellView":"form"},"source":["#@title feature 화"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CLaXBwWU_1z"},"source":["from tqdm import tqdm\n","import numpy as np\n","# 파라미터로 입력받은 파일에 저장된 단어 리스트를 딕셔너리 형태로 저장\n","def load_vocab(f_name):\n","    vocab_file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n","    print(\"{} vocab file loading...\".format(f_name))\n","\n","    # default 요소가 저장된 딕셔너리 생성\n","    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n","\n","    # 시작 인덱스 번호 저장\n","    index = len(symbol2idx)\n","    for line in tqdm(vocab_file.readlines()):\n","        symbol = line.strip()\n","        symbol2idx[symbol] = index\n","        idx2symbol[index]= symbol\n","        index+=1\n","\n","    return symbol2idx, idx2symbol\n","\n","# 입력 데이터를 고정 길이의 벡터로 표현하기 위한 함수\n","def convert_data2feature(data, symbol2idx, max_length=None):\n","    # 고정 길이의 0 벡터 생성\n","    feature = np.zeros(shape=(max_length), dtype=np.int)\n","    # 입력 문장을 공백 기준으로 split\n","    words = data.split()\n","\n","    for idx, word in enumerate(words[:max_length]):\n","        if word in symbol2idx.keys():\n","            feature[idx] = symbol2idx[word]\n","        else:\n","            feature[idx] = symbol2idx[\"<UNK>\"]\n","    return feature\n","\n","# 파라미터로 입력받은 파일로부터 tensor객체 생성\n","def load_data(config, f_name, word2idx, tag2idx):\n","    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n","\n","    # return할 문장/라벨 리스트 생성\n","    indexing_inputs, indexing_tags = [], []\n","\n","    print(\"{} file loading...\".format(f_name))\n","\n","    # 실제 데이터는 아래와 같은 형태를 가짐\n","    # 문장 \\t 태그\n","    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n","    for line in tqdm(file.readlines()):\n","        try:\n","            id, sentence, tags = line.strip().split('\\t')\n","        except:\n","            id, sentence = line.strip().split('\\t')\n","        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n","        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n","\n","        indexing_inputs.append(input_sentence)\n","        indexing_tags.append(indexing_tag)\n","    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n","    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n","\n","    return indexing_inputs, indexing_tags\n","\n","# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjDgtr7Xm5Tr"},"source":["# 새 섹션"]},{"cell_type":"markdown","metadata":{"id":"6RE5-RfyY2ra"},"source":["##input sentence로 feature 만들기"]},{"cell_type":"code","metadata":{"id":"reU2B2ekYmKD"},"source":["def bi_gram_feature(dic, sentence, labels = None, max_length = None):\n","  #dic = [date, time, org, loc, per]으로, 각 리스트는 np 행렬로 이루어져있음\n","  #NER = np.zeros(5) #순서대로 date, time, org, loc, per 순으로 원핫 인코딩을 리턴\n","  \n","  numbers = ['0','1','2','3','4','5','6','7','8','9']\n","  \n","  sentence = sentence.split()\n","\n","  for idx, word in enumerate(sentence):\n","    if word != '<SP>' and word.isupper():\n","      sentence[idx] = word.lower()\n","    elif word in numbers:\n","      sentence[idx] = 'N'\n","  \n","  sentence.insert(0,'<SP>')\n","\n","  bi_sentence = list(zip(*[sentence[i:] for i in range(2)]))\n","  # bi_sentence = [:len(bi_sentence)-1]\n","  \n","  NER_list = []\n","  for bi_gram in bi_sentence[:120]:\n","    \n","    NER = np.zeros(5)\n","    word2 = bi_gram[0]+bi_gram[1]\n","    \n","    # LOC, ORG, PER, DT, TI\n","    for idx, name in enumerate(name_list):\n","      if word2 in bi_dict[name]:\n","        NER[idx] = 1\n","        \n","    NER_list.append(NER)\n","\n","  padding_size = max_length - len(NER_list)\n","  for i in range(padding_size):\n","    NER_list.append(np.zeros(5))\n","    \n","  return NER_list\n","\n","def tri_gram_feature(dic, sentence, labels = None, max_length = None):\n","  #dic = [date, time, org, loc, per]으로, 각 리스트는 np 행렬로 이루어져있음\n","  #NER = np.zeros(5) #순서대로 date, time, org, loc, per 순으로 원핫 인코딩을 리턴\n","  \n","  numbers = ['0','1','2','3','4','5','6','7','8','9']\n","  \n","  sentence = sentence.split()\n","\n","  for idx, word in enumerate(sentence):\n","    if word != '<SP>' and word.isupper():\n","      sentence[idx] = word.lower()\n","    elif word in numbers:\n","      sentence[idx] = 'N'\n","  \n","  sentence.insert(0,'<SP>')\n","  sentence.append('<SP>')\n","\n","  bi_sentence = list(zip(*[sentence[i:] for i in range(3)]))\n","  NER_list = []\n","  for bi_gram in bi_sentence[:120]:\n","    \n","    NER = np.zeros(5)\n","    word2 = bi_gram[0]+bi_gram[1]+bi_gram[2]\n","    \n","    # LOC, ORG, PER, DT, TI\n","    for idx, name in enumerate(name_list):\n","      if word2 in tri_dict[name]:\n","        NER[idx] = 1\n","        \n","    NER_list.append(NER)\n","\n","  padding_size = max_length - len(NER_list)\n","  for i in range(padding_size):\n","    NER_list.append(np.zeros(5))\n","    \n","  return NER_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ahkts2MNaKqi"},"source":["#Train and Test"]},{"cell_type":"code","metadata":{"id":"HfKUI3mH4wqh"},"source":["from torch.utils.data import (DataLoader, TensorDataset)\n","import torch.optim as optim\n","\n","def train(config):\n","    # 모델 객체 생성\n","    # 단어 딕셔너리 생성\n","    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n","    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n","    # 데이터 Load\n","    train_input_features, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n","    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n","\n","    # train_input_features, train_tags = feature_dict['eumjeol.npy'], feature_dict['tag.npy']\n","    # test_input_features, test_tags = feature_dict['test_inputs.npy'], feature_dict['test_tag.npy']\n","\n","    gram = torch.cat([feature_dict['trigram_train_100_200.npy'], feature_dict['bigram_train_50_200.npy']], dim=-1)\n","    test = torch.cat([feature_dict['trigram_test_100_200.npy'], feature_dict['bigram_test_50_200.npy']], dim=-1)\n","    # train_input_ngram_features = feature_dict['trigram_train.npy']\n","    # test_input_ngram_features = feature_dict['trigram_test.npy']\n","    train_input_ngram_features = gram\n","    test_input_ngram_features = test\n","    model = RNN_CRF(config).cuda()\n","    \n","    # 불러온 데이터를 TensorDataset 객체로 변환\n","\n","    train_features = TensorDataset(train_input_features, train_tags, feature_dict['pumsa_onehot_200.npy'],  train_input_ngram_features)\n","    # train_features = TensorDataset(train_input_features, train_tags, jamo_feature)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    test_features = TensorDataset(test_input_features, test_tags, feature_dict['pumsa_onehot_200_test.npy'], test_input_ngram_features)\n","    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n","\n","    # # 모델을 학습하기위한 optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","    accuracy_list = []\n","    for epoch in range(config[\"epoch\"]):\n","        model.train()\n","        losses = []\n","        for step, batch in enumerate(train_dataloader):\n","            # .cuda()를 이용하여 메모리에 업로드\n","            batch = tuple(t.cuda() for t in batch)\n","            input_features, labels, pumsa, ngram_features = batch\n","            # loss 계산\n","            loss = model(input_features, labels, pumsa, ngram_features)\n","\n","            # 변화도 초기화\n","            optimizer.zero_grad()\n","\n","            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n","            loss.backward()\n","\n","            # 모델 내부 각 매개변수 가중치 갱신\n","            optimizer.step()\n","\n","            if (step + 1) % 50 == 0:\n","                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n","            losses.append(loss.data.item())\n","\n","\n","\n","        print(\"Average Loss : {}\".format(np.mean(losses)))\n","\n","        # 모델 저장\n","        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n","\n","        do_test(model, test_dataloader, idx2tag)\n","\n","\n","\n","def test(config):\n","    # 모델 객체 생성\n","    model = RNN_CRF(config).cuda()\n","    # 단어 딕셔너리 생성\n","    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n","    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n","\n","    # train_input_ngram_features = feature_dict['trigram_train.npy']\n","    # test_input_ngram_features = feature_dict['trigram_test.npy']\n","\n","    # for i in range(1, 21):\n","    #   config['trained_model_name'] = \"epoch_{}.pt\".format(i)\n","      # 저장된 가중치 Load\n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n","\n","    test_input_ngram_features = torch.cat([feature_dict['trigram_test_100.npy'], feature_dict['bigram_test_50.npy']], dim=-1)\n","    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n","    # 데이터 Load\n","\n","    # 불러온 데이터를 TensorDataset 객체로 변환\n","    test_features = TensorDataset(test_input_features, test_tags, feature_dict['pumsa_onehot_test.npy'], test_input_ngram_features)\n","    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n","    # 평가 함수 호출\n","    do_test(model, test_dataloader, idx2tag)\n","\n","def do_test(model, test_dataloader, idx2tag):\n","    model.eval()\n","    predicts, answers, outputs = [], [], []\n","    for step, batch in enumerate(test_dataloader):\n","        # .cuda() 함수를 이용하요 메모리에 업로드\n","        batch = tuple(t.cuda() for t in batch)\n","\n","        # 데이터를 각 변수에 저장\n","        input_features, labels, pumsa, ngram_features = batch\n","\n","        # 예측 라벨 출력\n","        output = model(input_features, pumsa=pumsa, inputs_ngram = ngram_features)\n","        outputs.append(output)\n","        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n","        for idx, answer in enumerate(tensor2list(labels)):\n","            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n","            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n","    \n","    # 성능 평가\n","    print(classification_report(answers, predicts))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QuqfrxJuaFih"},"source":["#MAIN"]},{"cell_type":"code","metadata":{"id":"QXHsFV-z4zZc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606969538332,"user_tz":-540,"elapsed":773590,"user":{"displayName":"박우진","photoUrl":"","userId":"05832430705160492475"}},"outputId":"fb05cd52-7ccd-4d12-974e-60608dbafcf0"},"source":["##########################################################\n","#                                                        #\n","#        평가 기준이 되는 지표는 Macro F1 Score          #\n","#           제출 포맷은 id \\t predict_tag                #\n","#            25 \\t B_PS I_PS <SP> O O O ...              #\n","#                                                        #\n","##########################################################\n","\n","\n","import os\n","if(__name__==\"__main__\"):\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    \n","    output_dir = os.path.join(output_dir, \"200\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    \n","    config = {\"mode\": \"train\",\n","              \"train_file\":\"ner_train.txt\",\n","              \"dev_file\": \"ner_dev.txt\",\n","              \"word_vocab_file\":\"vocab.txt\",\n","              \"tag_vocab_file\":\"tag_vocab.txt\",\n","              \"trained_model_name\":\"epoch_{}.pt\".format(11),\n","              \"output_dir_path\":output_dir,\n","              \"word_vocab_size\":2160,\n","              \"number_of_tags\": 14,\n","              \"hidden_size\": 100,\n","              \"dropout\":0.2,\n","              \"embedding_size\":100,\n","              \"max_length\": 200,\n","              \"batch_size\":64,\n","              \"epoch\":20,\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vocab.txt vocab file loading...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2158/2158 [00:00<00:00, 347072.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["tag_vocab.txt vocab file loading...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12/12 [00:00<00:00, 50181.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["ner_train.txt file loading...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7319/7319 [00:00<00:00, 20037.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["ner_dev.txt file loading...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 995/995 [00:00<00:00, 12427.73it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["50 step processed.. current loss : 19.182981491088867\n","100 step processed.. current loss : 9.986862182617188\n","Average Loss : 30.43329502603282\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          DT       0.69      0.66      0.67       624\n","          LC       0.65      0.65      0.65       537\n","          OG       0.55      0.39      0.45       973\n","          PS       0.70      0.66      0.68       742\n","          TI       0.09      0.08      0.09        95\n","\n","   micro avg       0.63      0.55      0.59      2971\n","   macro avg       0.54      0.49      0.51      2971\n","weighted avg       0.62      0.55      0.58      2971\n","\n","50 step processed.. current loss : 5.7325439453125\n","100 step processed.. current loss : 6.237150192260742\n","Average Loss : 6.650182591313901\n","              precision    recall  f1-score   support\n","\n","          DT       0.80      0.75      0.78       624\n","          LC       0.77      0.72      0.74       537\n","          OG       0.63      0.65      0.64       973\n","          PS       0.83      0.76      0.79       742\n","          TI       0.51      0.49      0.50        95\n","\n","   micro avg       0.73      0.70      0.72      2971\n","   macro avg       0.71      0.67      0.69      2971\n","weighted avg       0.74      0.70      0.72      2971\n","\n","50 step processed.. current loss : 4.698871612548828\n","100 step processed.. current loss : 4.303911209106445\n","Average Loss : 4.561772837846175\n","              precision    recall  f1-score   support\n","\n","          DT       0.81      0.82      0.82       624\n","          LC       0.79      0.76      0.78       537\n","          OG       0.73      0.70      0.71       973\n","          PS       0.87      0.79      0.82       742\n","          TI       0.71      0.75      0.73        95\n","\n","   micro avg       0.79      0.76      0.77      2971\n","   macro avg       0.78      0.76      0.77      2971\n","weighted avg       0.79      0.76      0.78      2971\n","\n","50 step processed.. current loss : 3.5972938537597656\n","100 step processed.. current loss : 3.3338050842285156\n","Average Loss : 3.4760945237201195\n","              precision    recall  f1-score   support\n","\n","          DT       0.83      0.81      0.82       624\n","          LC       0.79      0.80      0.80       537\n","          OG       0.77      0.66      0.71       973\n","          PS       0.88      0.81      0.85       742\n","          TI       0.76      0.75      0.76        95\n","\n","   micro avg       0.81      0.76      0.79      2971\n","   macro avg       0.81      0.77      0.79      2971\n","weighted avg       0.82      0.76      0.79      2971\n","\n","50 step processed.. current loss : 2.6774845123291016\n","100 step processed.. current loss : 2.3526153564453125\n","Average Loss : 2.792271010772042\n","              precision    recall  f1-score   support\n","\n","          DT       0.76      0.76      0.76       624\n","          LC       0.82      0.74      0.78       537\n","          OG       0.76      0.65      0.71       973\n","          PS       0.89      0.80      0.84       742\n","          TI       0.79      0.80      0.80        95\n","\n","   micro avg       0.81      0.73      0.77      2971\n","   macro avg       0.81      0.75      0.78      2971\n","weighted avg       0.81      0.73      0.77      2971\n","\n","50 step processed.. current loss : 2.6108531951904297\n","100 step processed.. current loss : 2.7508907318115234\n","Average Loss : 2.285529940024666\n","              precision    recall  f1-score   support\n","\n","          DT       0.87      0.82      0.85       624\n","          LC       0.79      0.82      0.81       537\n","          OG       0.78      0.71      0.74       973\n","          PS       0.90      0.81      0.85       742\n","          TI       0.83      0.77      0.80        95\n","\n","   micro avg       0.83      0.78      0.80      2971\n","   macro avg       0.83      0.79      0.81      2971\n","weighted avg       0.83      0.78      0.80      2971\n","\n","50 step processed.. current loss : 2.6222848892211914\n","100 step processed.. current loss : 1.8870830535888672\n","Average Loss : 1.9336810060169385\n","              precision    recall  f1-score   support\n","\n","          DT       0.86      0.81      0.84       624\n","          LC       0.85      0.77      0.80       537\n","          OG       0.78      0.72      0.75       973\n","          PS       0.90      0.82      0.86       742\n","          TI       0.82      0.80      0.81        95\n","\n","   micro avg       0.84      0.77      0.80      2971\n","   macro avg       0.84      0.78      0.81      2971\n","weighted avg       0.84      0.77      0.81      2971\n","\n","50 step processed.. current loss : 2.1367721557617188\n","100 step processed.. current loss : 2.19338321685791\n","Average Loss : 1.633855094080386\n","              precision    recall  f1-score   support\n","\n","          DT       0.87      0.81      0.84       624\n","          LC       0.81      0.80      0.80       537\n","          OG       0.80      0.65      0.72       973\n","          PS       0.90      0.79      0.84       742\n","          TI       0.87      0.76      0.81        95\n","\n","   micro avg       0.84      0.75      0.79      2971\n","   macro avg       0.85      0.76      0.80      2971\n","weighted avg       0.84      0.75      0.79      2971\n","\n","50 step processed.. current loss : 0.8354148864746094\n","100 step processed.. current loss : 1.2765212059020996\n","Average Loss : 1.4041187073873438\n","              precision    recall  f1-score   support\n","\n","          DT       0.88      0.81      0.84       624\n","          LC       0.81      0.83      0.82       537\n","          OG       0.77      0.73      0.75       973\n","          PS       0.88      0.83      0.86       742\n","          TI       0.86      0.76      0.80        95\n","\n","   micro avg       0.83      0.79      0.81      2971\n","   macro avg       0.84      0.79      0.81      2971\n","weighted avg       0.83      0.79      0.81      2971\n","\n","50 step processed.. current loss : 1.3265929222106934\n","100 step processed.. current loss : 1.0121145248413086\n","Average Loss : 1.2140148375345312\n","              precision    recall  f1-score   support\n","\n","          DT       0.84      0.79      0.82       624\n","          LC       0.83      0.80      0.82       537\n","          OG       0.77      0.72      0.74       973\n","          PS       0.87      0.83      0.85       742\n","          TI       0.83      0.81      0.82        95\n","\n","   micro avg       0.82      0.78      0.80      2971\n","   macro avg       0.83      0.79      0.81      2971\n","weighted avg       0.82      0.78      0.80      2971\n","\n","50 step processed.. current loss : 1.1802444458007812\n","100 step processed.. current loss : 0.6825308799743652\n","Average Loss : 1.0395885387192603\n","              precision    recall  f1-score   support\n","\n","          DT       0.88      0.83      0.85       624\n","          LC       0.81      0.82      0.82       537\n","          OG       0.73      0.74      0.74       973\n","          PS       0.90      0.83      0.86       742\n","          TI       0.84      0.80      0.82        95\n","\n","   micro avg       0.82      0.80      0.81      2971\n","   macro avg       0.83      0.80      0.82      2971\n","weighted avg       0.82      0.80      0.81      2971\n","\n","50 step processed.. current loss : 0.8219480514526367\n","100 step processed.. current loss : 0.6762721538543701\n","Average Loss : 0.9932326415310735\n","              precision    recall  f1-score   support\n","\n","          DT       0.83      0.82      0.83       624\n","          LC       0.79      0.85      0.82       537\n","          OG       0.75      0.71      0.73       973\n","          PS       0.86      0.85      0.85       742\n","          TI       0.87      0.80      0.84        95\n","\n","   micro avg       0.80      0.80      0.80      2971\n","   macro avg       0.82      0.81      0.81      2971\n","weighted avg       0.80      0.80      0.80      2971\n","\n","50 step processed.. current loss : 1.0674433708190918\n","100 step processed.. current loss : 1.32722008228302\n","Average Loss : 0.9158906262853871\n","              precision    recall  f1-score   support\n","\n","          DT       0.84      0.79      0.82       624\n","          LC       0.79      0.80      0.80       537\n","          OG       0.77      0.72      0.75       973\n","          PS       0.88      0.82      0.85       742\n","          TI       0.85      0.77      0.81        95\n","\n","   micro avg       0.82      0.78      0.80      2971\n","   macro avg       0.83      0.78      0.80      2971\n","weighted avg       0.82      0.78      0.80      2971\n","\n","50 step processed.. current loss : 0.5013355612754822\n","100 step processed.. current loss : 0.8017043471336365\n","Average Loss : 0.8496037630931191\n","              precision    recall  f1-score   support\n","\n","          DT       0.85      0.80      0.82       624\n","          LC       0.79      0.84      0.81       537\n","          OG       0.74      0.71      0.73       973\n","          PS       0.89      0.82      0.85       742\n","          TI       0.81      0.73      0.77        95\n","\n","   micro avg       0.81      0.78      0.79      2971\n","   macro avg       0.82      0.78      0.80      2971\n","weighted avg       0.81      0.78      0.79      2971\n","\n","50 step processed.. current loss : 0.8221167325973511\n","100 step processed.. current loss : 1.0919396877288818\n","Average Loss : 0.8160540274951769\n","              precision    recall  f1-score   support\n","\n","          DT       0.87      0.81      0.84       624\n","          LC       0.80      0.82      0.81       537\n","          OG       0.75      0.71      0.73       973\n","          PS       0.88      0.83      0.85       742\n","          TI       0.83      0.77      0.80        95\n","\n","   micro avg       0.82      0.78      0.80      2971\n","   macro avg       0.83      0.79      0.81      2971\n","weighted avg       0.82      0.78      0.80      2971\n","\n","50 step processed.. current loss : 0.5002992749214172\n","100 step processed.. current loss : 0.652119517326355\n","Average Loss : 0.6719817837943202\n","              precision    recall  f1-score   support\n","\n","          DT       0.85      0.80      0.82       624\n","          LC       0.79      0.82      0.81       537\n","          OG       0.76      0.71      0.74       973\n","          PS       0.88      0.84      0.86       742\n","          TI       0.77      0.76      0.76        95\n","\n","   micro avg       0.81      0.78      0.80      2971\n","   macro avg       0.81      0.79      0.80      2971\n","weighted avg       0.81      0.78      0.80      2971\n","\n","50 step processed.. current loss : 0.5826749801635742\n","100 step processed.. current loss : 0.6924384832382202\n","Average Loss : 0.5667398154735566\n","              precision    recall  f1-score   support\n","\n","          DT       0.86      0.81      0.83       624\n","          LC       0.78      0.84      0.81       537\n","          OG       0.75      0.71      0.73       973\n","          PS       0.89      0.85      0.87       742\n","          TI       0.83      0.77      0.80        95\n","\n","   micro avg       0.81      0.79      0.80      2971\n","   macro avg       0.82      0.80      0.81      2971\n","weighted avg       0.82      0.79      0.80      2971\n","\n","50 step processed.. current loss : 0.22856952250003815\n","100 step processed.. current loss : 0.575645923614502\n","Average Loss : 0.6068444473587948\n","              precision    recall  f1-score   support\n","\n","          DT       0.83      0.81      0.82       624\n","          LC       0.78      0.84      0.81       537\n","          OG       0.70      0.76      0.73       973\n","          PS       0.90      0.81      0.85       742\n","          TI       0.85      0.82      0.83        95\n","\n","   micro avg       0.79      0.80      0.79      2971\n","   macro avg       0.81      0.81      0.81      2971\n","weighted avg       0.80      0.80      0.80      2971\n","\n","50 step processed.. current loss : 0.5242723226547241\n","100 step processed.. current loss : 0.6976830959320068\n","Average Loss : 0.6302231697932533\n","              precision    recall  f1-score   support\n","\n","          DT       0.84      0.82      0.83       624\n","          LC       0.81      0.82      0.82       537\n","          OG       0.75      0.72      0.74       973\n","          PS       0.86      0.86      0.86       742\n","          TI       0.84      0.80      0.82        95\n","\n","   micro avg       0.81      0.80      0.80      2971\n","   macro avg       0.82      0.81      0.81      2971\n","weighted avg       0.81      0.80      0.80      2971\n","\n","50 step processed.. current loss : 0.6431004405021667\n","100 step processed.. current loss : 0.8409150838851929\n","Average Loss : 0.5104280834612639\n","              precision    recall  f1-score   support\n","\n","          DT       0.86      0.81      0.84       624\n","          LC       0.80      0.83      0.81       537\n","          OG       0.72      0.72      0.72       973\n","          PS       0.87      0.87      0.87       742\n","          TI       0.80      0.78      0.79        95\n","\n","   micro avg       0.80      0.80      0.80      2971\n","   macro avg       0.81      0.80      0.81      2971\n","weighted avg       0.80      0.80      0.80      2971\n","\n"],"name":"stdout"}]}]}